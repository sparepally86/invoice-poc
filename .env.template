# .env.template - copy to .env locally (do NOT commit .env)
# Provider selection
LLM_PROVIDER=noop          # noop | openai | local
VECTOR_PROVIDER=inmemory   # inmemory | pinecone | weaviate

# OpenAI (if using OpenAI)
OPENAI_API_KEY=
OPENAI_API_BASE=https://api.openai.com/v1

# Pinecone (if using Pinecone)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=
PINECONE_INDEX_NAME=invoice-poc

# Weaviate (if using Weaviate self-host or hosted)
WEAVIATE_URL=
WEAVIATE_API_KEY=

# Local LLM (if using self-hosted/ollama/llama)
LOCAL_LLM_URL=http://localhost:11434
LOCAL_LLM_MODEL=llama2

# Security / telemetry
TELEMETRY_WRITE=true

# Rate limits for LLM (defaults exist)
LLM_RATE_LIMIT_PER_MINUTE=60
LLM_RATE_LIMIT_CAPACITY=60

# Deployment marker (optional)
DEPLOY_ENV=development    # development | staging | production
